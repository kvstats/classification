{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction of Debt Crisis Events\n",
        "\n",
        "Inspired by Manassee-Roubini (2009), the purpose of this analysis is to investigate the set of economic and political conditions that are associated with a likely occurrence of a debt crisis. \n",
        "\n",
        "\n",
        "## Reference\n",
        "Manasse, Paolo & Roubini, Nouriel, 2009. \"\"Rules of thumb\" for sovereign debt crises,\" Journal of International Economics, Elsevier, vol. 78(2), pages 192-205, July."
      ],
      "metadata": {
        "id": "5O_CGmJsPFwI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNp-H7EXLAv_"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7DdPzpGLAwF",
        "outputId": "bd0d515e-6351-41c2-c72a-504747fcdc77"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Country</th>\n",
              "      <th>year</th>\n",
              "      <th>gfn2gdp</th>\n",
              "      <th>efn2gdp</th>\n",
              "      <th>FCD2GDweo</th>\n",
              "      <th>grossdebt2gdpweo</th>\n",
              "      <th>externdebt2gdp</th>\n",
              "      <th>credit2privat</th>\n",
              "      <th>stdebt2extdebt</th>\n",
              "      <th>CA2gdp</th>\n",
              "      <th>debtserv2res</th>\n",
              "      <th>IntExp2rev</th>\n",
              "      <th>interExtdeb2gdp</th>\n",
              "      <th>debtserv2expor</th>\n",
              "      <th>crisis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Albania</td>\n",
              "      <td>1990</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-2.690549</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Albania</td>\n",
              "      <td>1991</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.955623</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-4.312943</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.139587</td>\n",
              "      <td>2.180682</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Albania</td>\n",
              "      <td>1996</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>27.356787</td>\n",
              "      <td>3.832701</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.968338</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.208382</td>\n",
              "      <td>2.692118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Albania</td>\n",
              "      <td>1997</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>26.352253</td>\n",
              "      <td>3.763376</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-7.893714</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.406013</td>\n",
              "      <td>8.060182</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Albania</td>\n",
              "      <td>1999</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>34.046757</td>\n",
              "      <td>0.711837</td>\n",
              "      <td>39.140972</td>\n",
              "      <td>3.837488</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-6.943784</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33.784084</td>\n",
              "      <td>0.605327</td>\n",
              "      <td>9.086977</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Country  year  gfn2gdp  efn2gdp  FCD2GDweo  grossdebt2gdpweo  \\\n",
              "0  Albania  1990      NaN      NaN        NaN               NaN   \n",
              "1  Albania  1991      NaN      NaN        NaN               NaN   \n",
              "2  Albania  1996      NaN      NaN        NaN               NaN   \n",
              "3  Albania  1997      NaN      NaN        NaN               NaN   \n",
              "4  Albania  1999      NaN      NaN  34.046757          0.711837   \n",
              "\n",
              "   externdebt2gdp  credit2privat  stdebt2extdebt    CA2gdp  debtserv2res  \\\n",
              "0        0.000000            NaN             NaN -2.690549           NaN   \n",
              "1        3.955623            NaN             0.0 -4.312943           NaN   \n",
              "2       27.356787       3.832701             0.0 -1.968338           NaN   \n",
              "3       26.352253       3.763376             0.0 -7.893714           NaN   \n",
              "4       39.140972       3.837488             0.0 -6.943784           NaN   \n",
              "\n",
              "   IntExp2rev  interExtdeb2gdp  debtserv2expor  crisis  \n",
              "0         NaN         0.000000             NaN       0  \n",
              "1         NaN         0.139587        2.180682       1  \n",
              "2         NaN         0.208382        2.692118       0  \n",
              "3         NaN         0.406013        8.060182       1  \n",
              "4   33.784084         0.605327        9.086977       0  "
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read in crisis data\n",
        "crisis = pd.read_excel(\"crisisdata.xls\")\n",
        "crisis.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XF7QZOlLAwH"
      },
      "source": [
        "The variable descriptions are as follows: \n",
        "- grossdebt2gdpweo: Gross debt (%GDP) \n",
        "- efn2gdp: external financing need EFN (% GDP) \n",
        "- externdebt2gdp: external debt (%GDP) \n",
        "- gfn2gdp: gross financing need GFN (% GDP) \n",
        "- interExtdeb2gdp: Interest on external debt (% GDP) \n",
        "- CA2gdp: Current account (% GDP) \n",
        "- debtserv2expor: Debt service/Exports (%) \n",
        "- credit2privat: Credit to private sector (% GDP) \n",
        "- FCD2GDweo : Foreign currency denominated debt (% public debt) \n",
        "- IntExp2rev: Interest Expenditure/Government revenue (%) \n",
        "- debtserv2res: Debt services/Reserves (%) \n",
        "- stdebt2extdebt: Short term debt /External debt (%) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8zzcthYLAwI"
      },
      "source": [
        "### Part 1: Understanding the Data\n",
        "\n",
        "#### 1. Present descriptive statistics on the outcome variable. Comment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfcX33OBLAwI",
        "outputId": "225a5595-be22-4fae-95b2-d279ded1a06f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "             Count   Proportion\n",
            "Crisis = 0:  2715    93.42739\n",
            "Crisis = 1:  191     6.57261\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# descriptive statistics on outcome, crisis\n",
        "\n",
        "# frequency and proportion \n",
        "counts = crisis['crisis'].value_counts()\n",
        "prop = round(100*counts[1]/sum(counts),5)\n",
        "\n",
        "print(f'''\n",
        "             Count   Percentage\n",
        "Crisis = 0:  {counts[0]}    {100-prop}\n",
        "Crisis = 1:  {counts[1]}     {prop}\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITVcjGgQLAwJ"
      },
      "source": [
        "Here we can see the frequency and proportion of the outcome variable, crisis. We note that there is 2715 observations with crisis = 0, and 191 observations with crisis = 1, giving us percentages of 93.42% and 6.57%, respectively. \n",
        "\n",
        "This is interesting as we can see that the dataset may be imbalanced. An imbalance occurs when one or more classes have very low proportions in the training data as compared to the other classes.\n",
        "\n",
        "This can be a problem because typically, the minority class (in our case, debt crisis) is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class. This occurs in many real life situations such as this one, as well as rare/extreme event prediction.\n",
        "\n",
        "Now, let us split the data into two subsets, crisis and no crisis, then view the summary statistics by subgroup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-YB9I-kLAwJ",
        "outputId": "580dd424-6f2a-42ab-846e-734144021bc6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>gfn2gdp</th>\n",
              "      <th>efn2gdp</th>\n",
              "      <th>FCD2GDweo</th>\n",
              "      <th>grossdebt2gdpweo</th>\n",
              "      <th>externdebt2gdp</th>\n",
              "      <th>credit2privat</th>\n",
              "      <th>stdebt2extdebt</th>\n",
              "      <th>CA2gdp</th>\n",
              "      <th>debtserv2res</th>\n",
              "      <th>IntExp2rev</th>\n",
              "      <th>interExtdeb2gdp</th>\n",
              "      <th>debtserv2expor</th>\n",
              "      <th>crisis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2715.000000</td>\n",
              "      <td>1352.000000</td>\n",
              "      <td>1196.000000</td>\n",
              "      <td>994.000000</td>\n",
              "      <td>2239.000000</td>\n",
              "      <td>2011.000000</td>\n",
              "      <td>2377.000000</td>\n",
              "      <td>1526.000000</td>\n",
              "      <td>2490.000000</td>\n",
              "      <td>1331.000000</td>\n",
              "      <td>2434.000000</td>\n",
              "      <td>1600.000000</td>\n",
              "      <td>1511.000000</td>\n",
              "      <td>2715.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2003.892081</td>\n",
              "      <td>7.997088</td>\n",
              "      <td>16.344866</td>\n",
              "      <td>40.863213</td>\n",
              "      <td>0.488368</td>\n",
              "      <td>68.340746</td>\n",
              "      <td>76.887506</td>\n",
              "      <td>0.011155</td>\n",
              "      <td>-0.102589</td>\n",
              "      <td>114.728702</td>\n",
              "      <td>9.272912</td>\n",
              "      <td>2.464459</td>\n",
              "      <td>30.936303</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.978125</td>\n",
              "      <td>12.214868</td>\n",
              "      <td>34.805275</td>\n",
              "      <td>30.807023</td>\n",
              "      <td>0.343407</td>\n",
              "      <td>107.331112</td>\n",
              "      <td>62.320820</td>\n",
              "      <td>0.031036</td>\n",
              "      <td>11.236503</td>\n",
              "      <td>364.401479</td>\n",
              "      <td>9.426396</td>\n",
              "      <td>8.301201</td>\n",
              "      <td>37.935674</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1990.000000</td>\n",
              "      <td>-43.115822</td>\n",
              "      <td>-39.087891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-242.188065</td>\n",
              "      <td>-21.523167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1997.000000</td>\n",
              "      <td>1.762889</td>\n",
              "      <td>2.966474</td>\n",
              "      <td>12.673278</td>\n",
              "      <td>0.240723</td>\n",
              "      <td>24.179510</td>\n",
              "      <td>29.875000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.463280</td>\n",
              "      <td>20.891109</td>\n",
              "      <td>3.061741</td>\n",
              "      <td>0.711043</td>\n",
              "      <td>9.544457</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2004.000000</td>\n",
              "      <td>6.688442</td>\n",
              "      <td>9.224253</td>\n",
              "      <td>38.539671</td>\n",
              "      <td>0.426501</td>\n",
              "      <td>39.925087</td>\n",
              "      <td>55.413284</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.030251</td>\n",
              "      <td>48.610882</td>\n",
              "      <td>6.897833</td>\n",
              "      <td>1.330208</td>\n",
              "      <td>21.195290</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2011.000000</td>\n",
              "      <td>12.367588</td>\n",
              "      <td>19.066620</td>\n",
              "      <td>65.211071</td>\n",
              "      <td>0.655708</td>\n",
              "      <td>69.141159</td>\n",
              "      <td>115.425003</td>\n",
              "      <td>0.006335</td>\n",
              "      <td>3.563094</td>\n",
              "      <td>99.297058</td>\n",
              "      <td>12.094511</td>\n",
              "      <td>2.254286</td>\n",
              "      <td>39.068693</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2017.000000</td>\n",
              "      <td>110.963074</td>\n",
              "      <td>498.524750</td>\n",
              "      <td>127.325294</td>\n",
              "      <td>3.426656</td>\n",
              "      <td>1174.712769</td>\n",
              "      <td>421.024994</td>\n",
              "      <td>0.423879</td>\n",
              "      <td>64.969093</td>\n",
              "      <td>6108.248535</td>\n",
              "      <td>111.721611</td>\n",
              "      <td>126.146233</td>\n",
              "      <td>420.474579</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              year      gfn2gdp      efn2gdp   FCD2GDweo  grossdebt2gdpweo  \\\n",
              "count  2715.000000  1352.000000  1196.000000  994.000000       2239.000000   \n",
              "mean   2003.892081     7.997088    16.344866   40.863213          0.488368   \n",
              "std       7.978125    12.214868    34.805275   30.807023          0.343407   \n",
              "min    1990.000000   -43.115822   -39.087891    0.000000          0.000000   \n",
              "25%    1997.000000     1.762889     2.966474   12.673278          0.240723   \n",
              "50%    2004.000000     6.688442     9.224253   38.539671          0.426501   \n",
              "75%    2011.000000    12.367588    19.066620   65.211071          0.655708   \n",
              "max    2017.000000   110.963074   498.524750  127.325294          3.426656   \n",
              "\n",
              "       externdebt2gdp  credit2privat  stdebt2extdebt       CA2gdp  \\\n",
              "count     2011.000000    2377.000000     1526.000000  2490.000000   \n",
              "mean        68.340746      76.887506        0.011155    -0.102589   \n",
              "std        107.331112      62.320820        0.031036    11.236503   \n",
              "min          0.000000       0.000823        0.000000  -242.188065   \n",
              "25%         24.179510      29.875000        0.000000    -4.463280   \n",
              "50%         39.925087      55.413284        0.000000    -1.030251   \n",
              "75%         69.141159     115.425003        0.006335     3.563094   \n",
              "max       1174.712769     421.024994        0.423879    64.969093   \n",
              "\n",
              "       debtserv2res   IntExp2rev  interExtdeb2gdp  debtserv2expor  crisis  \n",
              "count   1331.000000  2434.000000      1600.000000     1511.000000  2715.0  \n",
              "mean     114.728702     9.272912         2.464459       30.936303     0.0  \n",
              "std      364.401479     9.426396         8.301201       37.935674     0.0  \n",
              "min      -21.523167     0.000000         0.000000        0.000000     0.0  \n",
              "25%       20.891109     3.061741         0.711043        9.544457     0.0  \n",
              "50%       48.610882     6.897833         1.330208       21.195290     0.0  \n",
              "75%       99.297058    12.094511         2.254286       39.068693     0.0  \n",
              "max     6108.248535   111.721611       126.146233      420.474579     0.0  "
            ]
          },
          "execution_count": 205,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# split into crisis =0 and crisis =1 check descriptive stats\n",
        "crisis0 = crisis[crisis['crisis']==0]\n",
        "crisis0.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2pzDEGkLAwK",
        "outputId": "6b13604b-7959-4b0b-bba2-bf248a6683ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>gfn2gdp</th>\n",
              "      <th>efn2gdp</th>\n",
              "      <th>FCD2GDweo</th>\n",
              "      <th>grossdebt2gdpweo</th>\n",
              "      <th>externdebt2gdp</th>\n",
              "      <th>credit2privat</th>\n",
              "      <th>stdebt2extdebt</th>\n",
              "      <th>CA2gdp</th>\n",
              "      <th>debtserv2res</th>\n",
              "      <th>IntExp2rev</th>\n",
              "      <th>interExtdeb2gdp</th>\n",
              "      <th>debtserv2expor</th>\n",
              "      <th>crisis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>191.000000</td>\n",
              "      <td>73.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>67.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>159.000000</td>\n",
              "      <td>125.000000</td>\n",
              "      <td>175.000000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>152.000000</td>\n",
              "      <td>155.000000</td>\n",
              "      <td>133.000000</td>\n",
              "      <td>191.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2000.900524</td>\n",
              "      <td>13.440484</td>\n",
              "      <td>18.299789</td>\n",
              "      <td>61.031404</td>\n",
              "      <td>0.620648</td>\n",
              "      <td>67.356581</td>\n",
              "      <td>40.256836</td>\n",
              "      <td>0.017571</td>\n",
              "      <td>-5.250366</td>\n",
              "      <td>339.898026</td>\n",
              "      <td>14.626044</td>\n",
              "      <td>3.519374</td>\n",
              "      <td>36.234540</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.509162</td>\n",
              "      <td>13.465229</td>\n",
              "      <td>21.344665</td>\n",
              "      <td>23.925811</td>\n",
              "      <td>0.434534</td>\n",
              "      <td>93.685837</td>\n",
              "      <td>46.865476</td>\n",
              "      <td>0.055291</td>\n",
              "      <td>9.133821</td>\n",
              "      <td>1300.707935</td>\n",
              "      <td>13.518846</td>\n",
              "      <td>9.619867</td>\n",
              "      <td>48.025205</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1990.000000</td>\n",
              "      <td>-6.123044</td>\n",
              "      <td>-8.445910</td>\n",
              "      <td>0.616486</td>\n",
              "      <td>0.058736</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.266927</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-73.215492</td>\n",
              "      <td>0.839750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1993.000000</td>\n",
              "      <td>3.965239</td>\n",
              "      <td>5.385578</td>\n",
              "      <td>46.273550</td>\n",
              "      <td>0.301422</td>\n",
              "      <td>29.416216</td>\n",
              "      <td>15.413882</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-8.502649</td>\n",
              "      <td>30.418505</td>\n",
              "      <td>4.898766</td>\n",
              "      <td>0.707463</td>\n",
              "      <td>10.092102</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1999.000000</td>\n",
              "      <td>9.401822</td>\n",
              "      <td>13.612785</td>\n",
              "      <td>57.873913</td>\n",
              "      <td>0.557577</td>\n",
              "      <td>50.268421</td>\n",
              "      <td>25.672920</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.977028</td>\n",
              "      <td>68.809967</td>\n",
              "      <td>11.035178</td>\n",
              "      <td>1.921634</td>\n",
              "      <td>19.847471</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2009.000000</td>\n",
              "      <td>20.196218</td>\n",
              "      <td>21.988723</td>\n",
              "      <td>82.255817</td>\n",
              "      <td>0.803563</td>\n",
              "      <td>68.053633</td>\n",
              "      <td>48.420927</td>\n",
              "      <td>0.007146</td>\n",
              "      <td>-1.309848</td>\n",
              "      <td>159.432846</td>\n",
              "      <td>20.418385</td>\n",
              "      <td>3.177070</td>\n",
              "      <td>43.539005</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2017.000000</td>\n",
              "      <td>61.179409</td>\n",
              "      <td>122.522301</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>2.264065</td>\n",
              "      <td>901.229187</td>\n",
              "      <td>312.117859</td>\n",
              "      <td>0.533167</td>\n",
              "      <td>26.701317</td>\n",
              "      <td>10538.450195</td>\n",
              "      <td>86.565895</td>\n",
              "      <td>86.766556</td>\n",
              "      <td>296.080353</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              year    gfn2gdp     efn2gdp   FCD2GDweo  grossdebt2gdpweo  \\\n",
              "count   191.000000  73.000000   76.000000   67.000000        118.000000   \n",
              "mean   2000.900524  13.440484   18.299789   61.031404          0.620648   \n",
              "std       8.509162  13.465229   21.344665   23.925811          0.434534   \n",
              "min    1990.000000  -6.123044   -8.445910    0.616486          0.058736   \n",
              "25%    1993.000000   3.965239    5.385578   46.273550          0.301422   \n",
              "50%    1999.000000   9.401822   13.612785   57.873913          0.557577   \n",
              "75%    2009.000000  20.196218   21.988723   82.255817          0.803563   \n",
              "max    2017.000000  61.179409  122.522301  100.000000          2.264065   \n",
              "\n",
              "       externdebt2gdp  credit2privat  stdebt2extdebt      CA2gdp  \\\n",
              "count      176.000000     159.000000      125.000000  175.000000   \n",
              "mean        67.356581      40.256836        0.017571   -5.250366   \n",
              "std         93.685837      46.865476        0.055291    9.133821   \n",
              "min          0.000000       1.266927        0.000000  -73.215492   \n",
              "25%         29.416216      15.413882        0.000000   -8.502649   \n",
              "50%         50.268421      25.672920        0.000000   -3.977028   \n",
              "75%         68.053633      48.420927        0.007146   -1.309848   \n",
              "max        901.229187     312.117859        0.533167   26.701317   \n",
              "\n",
              "       debtserv2res  IntExp2rev  interExtdeb2gdp  debtserv2expor  crisis  \n",
              "count     97.000000  152.000000       155.000000      133.000000   191.0  \n",
              "mean     339.898026   14.626044         3.519374       36.234540     1.0  \n",
              "std     1300.707935   13.518846         9.619867       48.025205     0.0  \n",
              "min        0.839750    0.000000         0.000000        0.000000     1.0  \n",
              "25%       30.418505    4.898766         0.707463       10.092102     1.0  \n",
              "50%       68.809967   11.035178         1.921634       19.847471     1.0  \n",
              "75%      159.432846   20.418385         3.177070       43.539005     1.0  \n",
              "max    10538.450195   86.565895        86.766556      296.080353     1.0  "
            ]
          },
          "execution_count": 206,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# crisis = 1\n",
        "crisis1 = crisis[crisis['crisis']==1]\n",
        "crisis1.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPevbHpnLAwK"
      },
      "source": [
        "Here we can see the summary statistics for both groups. As a sanity check, we can look to make sure that the values make sense intuitively, given the context of the question. Here we can see that the values for variables relating to debt are higher when we are in crisis, which makes sense. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTAX--HNLAwK"
      },
      "source": [
        "#### 2. Present descriptive statistics on the input variables depending on the default status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taoZE74qLAwL",
        "outputId": "e6fa1a09-0b87-4fbe-d188-bb4a39c9d52d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>gfn2gdp</th>\n",
              "      <th>efn2gdp</th>\n",
              "      <th>FCD2GDweo</th>\n",
              "      <th>grossdebt2gdpweo</th>\n",
              "      <th>externdebt2gdp</th>\n",
              "      <th>credit2privat</th>\n",
              "      <th>stdebt2extdebt</th>\n",
              "      <th>CA2gdp</th>\n",
              "      <th>debtserv2res</th>\n",
              "      <th>IntExp2rev</th>\n",
              "      <th>interExtdeb2gdp</th>\n",
              "      <th>debtserv2expor</th>\n",
              "      <th>crisis</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>2906.000000</td>\n",
              "      <td>1425.000000</td>\n",
              "      <td>1272.000000</td>\n",
              "      <td>1061.000000</td>\n",
              "      <td>2357.000000</td>\n",
              "      <td>2187.000000</td>\n",
              "      <td>2536.000000</td>\n",
              "      <td>1651.000000</td>\n",
              "      <td>2665.000000</td>\n",
              "      <td>1428.000000</td>\n",
              "      <td>2586.000000</td>\n",
              "      <td>1755.000000</td>\n",
              "      <td>1644.000000</td>\n",
              "      <td>2906.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2003.695458</td>\n",
              "      <td>8.275942</td>\n",
              "      <td>16.461670</td>\n",
              "      <td>42.136794</td>\n",
              "      <td>0.494990</td>\n",
              "      <td>68.261545</td>\n",
              "      <td>74.590867</td>\n",
              "      <td>0.011641</td>\n",
              "      <td>-0.440623</td>\n",
              "      <td>130.023818</td>\n",
              "      <td>9.587559</td>\n",
              "      <td>2.557628</td>\n",
              "      <td>31.364932</td>\n",
              "      <td>0.065726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.046797</td>\n",
              "      <td>12.335433</td>\n",
              "      <td>34.147759</td>\n",
              "      <td>30.802815</td>\n",
              "      <td>0.349616</td>\n",
              "      <td>106.278805</td>\n",
              "      <td>62.097167</td>\n",
              "      <td>0.033509</td>\n",
              "      <td>11.182149</td>\n",
              "      <td>490.704999</td>\n",
              "      <td>9.792534</td>\n",
              "      <td>8.428237</td>\n",
              "      <td>38.858821</td>\n",
              "      <td>0.247845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1990.000000</td>\n",
              "      <td>-43.115822</td>\n",
              "      <td>-39.087891</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-242.188065</td>\n",
              "      <td>-21.523167</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1997.000000</td>\n",
              "      <td>1.942951</td>\n",
              "      <td>3.176368</td>\n",
              "      <td>14.569268</td>\n",
              "      <td>0.242832</td>\n",
              "      <td>24.735479</td>\n",
              "      <td>28.180852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.672428</td>\n",
              "      <td>21.238802</td>\n",
              "      <td>3.125898</td>\n",
              "      <td>0.710925</td>\n",
              "      <td>9.571682</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2004.000000</td>\n",
              "      <td>6.803134</td>\n",
              "      <td>9.499497</td>\n",
              "      <td>40.013073</td>\n",
              "      <td>0.429259</td>\n",
              "      <td>40.758839</td>\n",
              "      <td>52.932198</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.270393</td>\n",
              "      <td>49.171139</td>\n",
              "      <td>7.089282</td>\n",
              "      <td>1.351801</td>\n",
              "      <td>21.059856</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2011.000000</td>\n",
              "      <td>12.658605</td>\n",
              "      <td>19.360725</td>\n",
              "      <td>67.044731</td>\n",
              "      <td>0.663545</td>\n",
              "      <td>68.904839</td>\n",
              "      <td>110.318752</td>\n",
              "      <td>0.006373</td>\n",
              "      <td>3.317413</td>\n",
              "      <td>102.077942</td>\n",
              "      <td>12.508049</td>\n",
              "      <td>2.335770</td>\n",
              "      <td>39.379719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>2017.000000</td>\n",
              "      <td>110.963074</td>\n",
              "      <td>498.524750</td>\n",
              "      <td>127.325294</td>\n",
              "      <td>3.426656</td>\n",
              "      <td>1174.712769</td>\n",
              "      <td>421.024994</td>\n",
              "      <td>0.533167</td>\n",
              "      <td>64.969093</td>\n",
              "      <td>10538.450195</td>\n",
              "      <td>111.721611</td>\n",
              "      <td>126.146233</td>\n",
              "      <td>420.474579</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              year      gfn2gdp      efn2gdp    FCD2GDweo  grossdebt2gdpweo  \\\n",
              "count  2906.000000  1425.000000  1272.000000  1061.000000       2357.000000   \n",
              "mean   2003.695458     8.275942    16.461670    42.136794          0.494990   \n",
              "std       8.046797    12.335433    34.147759    30.802815          0.349616   \n",
              "min    1990.000000   -43.115822   -39.087891     0.000000          0.000000   \n",
              "25%    1997.000000     1.942951     3.176368    14.569268          0.242832   \n",
              "50%    2004.000000     6.803134     9.499497    40.013073          0.429259   \n",
              "75%    2011.000000    12.658605    19.360725    67.044731          0.663545   \n",
              "max    2017.000000   110.963074   498.524750   127.325294          3.426656   \n",
              "\n",
              "       externdebt2gdp  credit2privat  stdebt2extdebt       CA2gdp  \\\n",
              "count     2187.000000    2536.000000     1651.000000  2665.000000   \n",
              "mean        68.261545      74.590867        0.011641    -0.440623   \n",
              "std        106.278805      62.097167        0.033509    11.182149   \n",
              "min          0.000000       0.000823        0.000000  -242.188065   \n",
              "25%         24.735479      28.180852        0.000000    -4.672428   \n",
              "50%         40.758839      52.932198        0.000000    -1.270393   \n",
              "75%         68.904839     110.318752        0.006373     3.317413   \n",
              "max       1174.712769     421.024994        0.533167    64.969093   \n",
              "\n",
              "       debtserv2res   IntExp2rev  interExtdeb2gdp  debtserv2expor       crisis  \n",
              "count   1428.000000  2586.000000      1755.000000     1644.000000  2906.000000  \n",
              "mean     130.023818     9.587559         2.557628       31.364932     0.065726  \n",
              "std      490.704999     9.792534         8.428237       38.858821     0.247845  \n",
              "min      -21.523167     0.000000         0.000000        0.000000     0.000000  \n",
              "25%       21.238802     3.125898         0.710925        9.571682     0.000000  \n",
              "50%       49.171139     7.089282         1.351801       21.059856     0.000000  \n",
              "75%      102.077942    12.508049         2.335770       39.379719     0.000000  \n",
              "max    10538.450195   111.721611       126.146233      420.474579     1.000000  "
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crisis.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnT-E3qRLAwL",
        "outputId": "c8addc60-d369-44c5-c40f-84ebabe7a87f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2906 entries, 0 to 2905\n",
            "Data columns (total 15 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Country           2906 non-null   object \n",
            " 1   year              2906 non-null   int64  \n",
            " 2   gfn2gdp           1425 non-null   float64\n",
            " 3   efn2gdp           1272 non-null   float64\n",
            " 4   FCD2GDweo         1061 non-null   float64\n",
            " 5   grossdebt2gdpweo  2357 non-null   float64\n",
            " 6   externdebt2gdp    2187 non-null   float64\n",
            " 7   credit2privat     2536 non-null   float64\n",
            " 8   stdebt2extdebt    1651 non-null   float64\n",
            " 9   CA2gdp            2665 non-null   float64\n",
            " 10  debtserv2res      1428 non-null   float64\n",
            " 11  IntExp2rev        2586 non-null   float64\n",
            " 12  interExtdeb2gdp   1755 non-null   float64\n",
            " 13  debtserv2expor    1644 non-null   float64\n",
            " 14  crisis            2906 non-null   int64  \n",
            "dtypes: float64(12), int64(2), object(1)\n",
            "memory usage: 340.7+ KB\n"
          ]
        }
      ],
      "source": [
        "crisis.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vXZMVtjLAwM"
      },
      "source": [
        "Using the .describe() function, we can see the summary statistics for the input variables. This function provides the count, mean, std, min, max, and quartiles. Moreover, the .info() function gives us information on the type of each variable, and if there are any missing values. We see here that most of the variables are numeric, with country the only categorical variable. We can deal with missing values and categorical variables next in the data preprocessing step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us9f1gBNLAwM"
      },
      "source": [
        "### Data Cleaning\n",
        "\n",
        "Next, I am going to preprocess the data by dealing with missing values and encoding the proper variables. In the next section, I will also standardize the data before passing it into our classifiers. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzF13qgXLAwM"
      },
      "source": [
        "#### Missing Values\n",
        "In order to deal with missing values, I will consider imputing by the process of linear interpolation. \n",
        "\n",
        "Linear interpolation is the technique of determining the values of the functions of any intermediate points when the values of two adjacent points are known. In essence, it is the estimation of an unknown value that falls within two known values.\n",
        "\n",
        "Because this process is forward linear, the first values for each variable will not be imputed. To overcome this, I will continue with mean imputation for the remaining missing values. As the name suggests, mean imputation is a method in which the mean of the observed values for each variable is computed and the missing values for that variable are imputed by this mean.\n",
        "\n",
        "**Note:** I am dealing with missing values here, however, the process as well as it's drawbacks will be described again in part 5 below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYXsKc7ILAwN",
        "outputId": "06fe07a4-376b-4f4b-ebe8-2870646eab98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-207-f4cb2727f35f>:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  crisis = crisis.fillna(crisis.mean())\n"
          ]
        }
      ],
      "source": [
        "# missing values\n",
        "crisis = crisis.interpolate()\n",
        "crisis = crisis.fillna(crisis.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMI77yEULAwN",
        "outputId": "cba67345-b834-4962-ad01-4305e7f9c865"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2906 entries, 0 to 2905\n",
            "Data columns (total 15 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Country           2906 non-null   object \n",
            " 1   year              2906 non-null   int64  \n",
            " 2   gfn2gdp           2906 non-null   float64\n",
            " 3   efn2gdp           2906 non-null   float64\n",
            " 4   FCD2GDweo         2906 non-null   float64\n",
            " 5   grossdebt2gdpweo  2906 non-null   float64\n",
            " 6   externdebt2gdp    2906 non-null   float64\n",
            " 7   credit2privat     2906 non-null   float64\n",
            " 8   stdebt2extdebt    2906 non-null   float64\n",
            " 9   CA2gdp            2906 non-null   float64\n",
            " 10  debtserv2res      2906 non-null   float64\n",
            " 11  IntExp2rev        2906 non-null   float64\n",
            " 12  interExtdeb2gdp   2906 non-null   float64\n",
            " 13  debtserv2expor    2906 non-null   float64\n",
            " 14  crisis            2906 non-null   int64  \n",
            "dtypes: float64(12), int64(2), object(1)\n",
            "memory usage: 340.7+ KB\n"
          ]
        }
      ],
      "source": [
        "crisis.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBgHGe7jLAwN"
      },
      "source": [
        "Here we can see that we now have no missing values. Next, we can encode our categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPzraX-gLAwO"
      },
      "source": [
        "#### Encoding\n",
        "\n",
        "As discussed in Exercise 2, encoding categorical variables is important as the way we represent our variables can have an effect on the way they interact with our methods used. As we saw in the previous question, we have only one categorical variable in the dataset, namely, Country.\n",
        "\n",
        "The process I will be using to encode the country variable is one hot encoding. As previously discussed, this method creates a new dummy variable for each level of the variable. Integer encoding is not a good choice here as the integer values have a natural ordered relationship between each other and machine learning algorithms may be able to understand and harness this relationship. In order words, it introduces an ordinal relationship to the variable. Since country is not ordered, it is best that we just create dummy variables. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTDssZ-SLAwO"
      },
      "outputs": [],
      "source": [
        "# encode country as categorical using OneHotEncoder\n",
        "# import function\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# define encoder\n",
        "onehotencoder = OneHotEncoder()\n",
        "\n",
        "# reformat 1-D country array to 2-D\n",
        "country_ohe = onehotencoder.fit_transform(crisis.Country.values.reshape(-1,1)).toarray()\n",
        "\n",
        "# add back into original data frame\n",
        "crisis_ohe = pd.DataFrame(country_ohe, columns = [\"Country_\"+str(int(i)) for i in range(country_ohe.shape[1])])\n",
        "crisis = pd.concat([crisis, crisis_ohe], axis=1)\n",
        "crisis = crisis.drop(['Country'], axis=1)\n",
        "\n",
        "# drop one of the dummy variables to avoid multicollinearity\n",
        "crisis = crisis.drop(['Country_0'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7BXmEZVLAwO",
        "outputId": "93bae93f-54fb-4eb0-8779-b5e6d00e2090"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>gfn2gdp</th>\n",
              "      <th>efn2gdp</th>\n",
              "      <th>FCD2GDweo</th>\n",
              "      <th>grossdebt2gdpweo</th>\n",
              "      <th>externdebt2gdp</th>\n",
              "      <th>credit2privat</th>\n",
              "      <th>stdebt2extdebt</th>\n",
              "      <th>CA2gdp</th>\n",
              "      <th>debtserv2res</th>\n",
              "      <th>...</th>\n",
              "      <th>Country_107</th>\n",
              "      <th>Country_108</th>\n",
              "      <th>Country_109</th>\n",
              "      <th>Country_110</th>\n",
              "      <th>Country_111</th>\n",
              "      <th>Country_112</th>\n",
              "      <th>Country_113</th>\n",
              "      <th>Country_114</th>\n",
              "      <th>Country_115</th>\n",
              "      <th>Country_116</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1990</td>\n",
              "      <td>8.424674</td>\n",
              "      <td>26.004948</td>\n",
              "      <td>40.327620</td>\n",
              "      <td>0.504628</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>73.924386</td>\n",
              "      <td>0.014401</td>\n",
              "      <td>-2.690549</td>\n",
              "      <td>223.078809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1991</td>\n",
              "      <td>8.424674</td>\n",
              "      <td>26.004948</td>\n",
              "      <td>40.327620</td>\n",
              "      <td>0.504628</td>\n",
              "      <td>3.955623</td>\n",
              "      <td>73.924386</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-4.312943</td>\n",
              "      <td>223.078809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1996</td>\n",
              "      <td>8.424674</td>\n",
              "      <td>26.004948</td>\n",
              "      <td>40.327620</td>\n",
              "      <td>0.504628</td>\n",
              "      <td>27.356787</td>\n",
              "      <td>3.832701</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.968338</td>\n",
              "      <td>223.078809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1997</td>\n",
              "      <td>8.424674</td>\n",
              "      <td>26.004948</td>\n",
              "      <td>40.327620</td>\n",
              "      <td>0.504628</td>\n",
              "      <td>26.352253</td>\n",
              "      <td>3.763376</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-7.893714</td>\n",
              "      <td>223.078809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1999</td>\n",
              "      <td>8.424674</td>\n",
              "      <td>26.004948</td>\n",
              "      <td>34.046757</td>\n",
              "      <td>0.711837</td>\n",
              "      <td>39.140972</td>\n",
              "      <td>3.837488</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-6.943784</td>\n",
              "      <td>223.078809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 130 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   year   gfn2gdp    efn2gdp  FCD2GDweo  grossdebt2gdpweo  externdebt2gdp  \\\n",
              "0  1990  8.424674  26.004948  40.327620          0.504628        0.000000   \n",
              "1  1991  8.424674  26.004948  40.327620          0.504628        3.955623   \n",
              "2  1996  8.424674  26.004948  40.327620          0.504628       27.356787   \n",
              "3  1997  8.424674  26.004948  40.327620          0.504628       26.352253   \n",
              "4  1999  8.424674  26.004948  34.046757          0.711837       39.140972   \n",
              "\n",
              "   credit2privat  stdebt2extdebt    CA2gdp  debtserv2res  ...  Country_107  \\\n",
              "0      73.924386        0.014401 -2.690549    223.078809  ...          0.0   \n",
              "1      73.924386        0.000000 -4.312943    223.078809  ...          0.0   \n",
              "2       3.832701        0.000000 -1.968338    223.078809  ...          0.0   \n",
              "3       3.763376        0.000000 -7.893714    223.078809  ...          0.0   \n",
              "4       3.837488        0.000000 -6.943784    223.078809  ...          0.0   \n",
              "\n",
              "   Country_108  Country_109  Country_110  Country_111  Country_112  \\\n",
              "0          0.0          0.0          0.0          0.0          0.0   \n",
              "1          0.0          0.0          0.0          0.0          0.0   \n",
              "2          0.0          0.0          0.0          0.0          0.0   \n",
              "3          0.0          0.0          0.0          0.0          0.0   \n",
              "4          0.0          0.0          0.0          0.0          0.0   \n",
              "\n",
              "   Country_113  Country_114  Country_115  Country_116  \n",
              "0          0.0          0.0          0.0          0.0  \n",
              "1          0.0          0.0          0.0          0.0  \n",
              "2          0.0          0.0          0.0          0.0  \n",
              "3          0.0          0.0          0.0          0.0  \n",
              "4          0.0          0.0          0.0          0.0  \n",
              "\n",
              "[5 rows x 130 columns]"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "crisis.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do7ubNQ3LAwP"
      },
      "source": [
        "Now we can see that we have new binary/dummy variables for each country. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLNUUz3yLAwP"
      },
      "source": [
        "### Part 2: Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fetLs4HqLAwP"
      },
      "source": [
        "#### 3. Why is it important to split the database into training and test sample? Consider a training sample with year<=2009 and a test sample with year>2009.\n",
        "\n",
        "It is important to split the data into training and testing sets in order to try and avoid problems such as underfitting or overfitting. Under fitting occurs when the data is unable to encapsulate the relations among data. For example, this can occur when we try to fit non-linear data with a linear model. Overfitting occurs when a model learns both the existing relations among data and noise.\n",
        "\n",
        "After training our model using the training set, we should test it on unseen data from the test set to make sure the model is not just memorizing specific patterns in the training set. We can also estimate how well our model is performing while using new inputs. This is important as we are interested in  different model performance metrics such as accuracy. \n",
        "\n",
        "In the context of this question, we split the train/test by year as we are also interested in time trend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmtzKPBtLAwQ"
      },
      "source": [
        "#### Train/Test Split\n",
        "\n",
        "Here we can consider a training sample with year<=2009 and a test sample with year>2009."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhrM9gSYLAwQ"
      },
      "outputs": [],
      "source": [
        "train = crisis[crisis['year']<=2009]\n",
        "test = crisis[crisis['year']>2009]\n",
        "\n",
        "X_train = train.drop('crisis', axis=1)\n",
        "X_test = test.drop('crisis', axis=1)\n",
        "y_train = train['crisis']\n",
        "y_test = test['crisis']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ghV13qlLAwQ"
      },
      "source": [
        "#### Scale Data\n",
        "\n",
        "Next, we want to scale the data before passing it into our classifiers as it can help the algorithm get trained well and faster. Scaling is a technique to make them closer to each other, i.e., make data points generalized so that the distance between them will be lower. \n",
        "\n",
        "For our purposes, we will be using the StandardScaler from sklearn preprocessing. This scaler standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "The standard score of a sample x is calculated as:\n",
        "$$\n",
        "z = \\frac{(x - \\mu)} {\\sigma}\n",
        "$$\n",
        "\n",
        "Where $\\mu$ is the mean of the training samples, and $\\sigma$ is the standard deviation of the training samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ronp09wWLAwR"
      },
      "outputs": [],
      "source": [
        "# scale the data \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# can also scale using MinMaxScaler, this will put values [0,1]\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scaler = MinMaxScaler()\n",
        "# X_train = scaler.fit_transform(X_train)\n",
        "# X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHaxsIKoLAwR"
      },
      "source": [
        "#### 4. Explain the procedure that you will be using to tune your model's parameters and find the accuracy of your model.\n",
        "\n",
        "In order to tune my model's parameters and find the accuracy of my model, I will be using a k-fold cross-validation procedure for hyperparameter tuning and examining AUC scores as a measure of accuracy. \n",
        "\n",
        "The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter. This grid is provided by the user and has all the values that we want to try combinations from in our search. It runs through all the different parameters that is fed into the parameter grid and produces the best combination of parameters, based on a scoring metric of your choice (accuracy, f1, etc). In our case, we are interested in AUC as our scoring metric. \n",
        "\n",
        "The cv parameter also allows us to specify the the number of cv folds for each combination of parameters. Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called $k$ that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. In k-fold cross validation, the training set is split into $k$ smaller sets. Then following procedure is followed for each of the k “folds”:\n",
        "\n",
        "- A model is trained using $k-1$ of the folds as training data;\n",
        "- The resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
        "\n",
        "Thus, using this technique, we are able to find the best parameters for our classifiers, i.e., tuning the model's parameters. For this assignment, I will proceed with using $k=10$ folds.\n",
        "\n",
        "Then, to evaluate model accuracy, we will examine the AUC scores. We can set this to be our scoring metric in the grid search as previously mentioned. \n",
        "\n",
        "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve. The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the true positive rate (TPR) against the false negative rate (FPR) at various threshold values and essentially separates the ‘signal’ from the ‘noise’. \n",
        "\n",
        "Note that the TPR is also called sensitivity and tells us what proportion of the positive class got correctly classified and is calculated as follows:\n",
        "\n",
        "$$\n",
        "TPR = \\frac{TP}{TP+FN}\n",
        "$$\n",
        "\n",
        "and FNR tells us what proportion of the positive class got incorrectly classified by the classifier:\n",
        "$$\n",
        "FNR = \\frac{FN}{TP+FN}\n",
        "$$\n",
        "\n",
        "Where TP = true positives, and FN = false negatives.\n",
        "\n",
        "The AUC then is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIwtvrqfLAwS"
      },
      "source": [
        "#### 5. Propose a method to deal with missing values\n",
        "\n",
        "In order to deal with missing values, I will consider imputing by the process of linear interpolation. \n",
        "\n",
        "Linear interpolation is the technique of determining the values of the functions of any intermediate points when the values of two adjacent points are known. In essence, it is the estimation of an unknown value that falls within two known values. This can be done by group, i.e, by country. \n",
        "\n",
        "Because this process is forward linear, the first values for each variable will not be imputed. To overcome this, I will continue with mean imputation for the remaining missing values. As the name suggests, mean imputation is a method in which the mean of the observed values for each variable is computed and the missing values for that variable are imputed by this mean.\n",
        "\n",
        "In practice, we would like to try and avoid mean imputation on the full dataset and there are many dangers that come with this process. For example, mean imputation does not preserve the relationships among variables. Moreover, it leads to an underestimate of the standard error. \n",
        "\n",
        "There are alternative methods for dealing with missing values. For example, if there is a variable with extreme missing values, we may decide to just drop the whole column. This is not ideal as we will be losing data. This is the same case with deleting rows/observations with missing data. More often than not, missing values can tell a story themselves. Another way to deal with missing values is to mean impute, but then create a new column that indicates whether or not a variable was imputed. There are many types of deterministic imputation methods that we could also explore, including logical imputation, historical (e.g. carry-forward) imputation, ratio and regression imputation. Some of these options may be costly in time and computation. Ultimately, one must be cautious when altering the original dataset.\n",
        "\n",
        "Please see the \"Data Cleaning\" section where the missing values were handled for the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seMiP1TqLAwT"
      },
      "source": [
        "#### 6. Using default values, construct the following classifiers to predict debt crisis events and summarize the information in a table. The criteria to measure the accuracy is the AUC.\n",
        "a. Penalized Logistic Classifier \\\n",
        "b. Decision Tree Classifier \\\n",
        "c. Random Forest classifier \\\n",
        "d. Boosting tree classifier \\\n",
        "e. K-NN classifier with K=5 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKK4i9JwLAwT"
      },
      "source": [
        "First, we will construct and train our classifiers using our training data. Then, we will calculate the AUC values and summarize information in a table:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh017V4yLAwU"
      },
      "source": [
        "#### Penalized Logistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyruibUPLAwU",
        "outputId": "2a3f585a-d199-49ef-f378-0cfffa134e7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Penalized Logistic Classifier Accuracy: 0.9534606205250596\n"
          ]
        }
      ],
      "source": [
        "# construct classifiers\n",
        "\n",
        "# penalized logistic classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "log = LogisticRegression()\n",
        "\n",
        "log_fit = log.fit(X_train, y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = log_fit.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "# print(\"Penalized Logistic Classifier Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfeq1qLzLAwV"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJB2HF20LAwV",
        "outputId": "ae3d3cd0-b648-44f9-e9bd-44247353af0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree Accuracy: 0.8162291169451074\n"
          ]
        }
      ],
      "source": [
        "# decision tree classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# train decision tree classifer\n",
        "dt_fit=dt.fit(X_train,y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "y_pred = dt_fit.predict(X_test)\n",
        "\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "# print(\"Decision Tree Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOrp3dqsLAwV"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5CA2erRLAwW",
        "outputId": "4e6f85c3-c947-448e-a86e-65a87bd93945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest Accuracy: 0.9534606205250596\n"
          ]
        }
      ],
      "source": [
        "# random forest classifer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf=RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X_train,y_train)\n",
        "\n",
        "y_pred=rf.predict(X_test)\n",
        "#print(\"Random Forest Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwlZ8VSkLAwW"
      },
      "source": [
        "#### Boosting Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN5dVU2VLAwX",
        "outputId": "3c309cfd-1aad-4a29-c4c7-b7a0730738ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradientBoostingClassifier()"
            ]
          },
          "execution_count": 305,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# boosting tree classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gb = GradientBoostingClassifier()\n",
        "gb.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhhywvQjLAwX"
      },
      "source": [
        "#### K-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcBbaw3qLAwY",
        "outputId": "6276b73c-6188-4029-d8d5-6e4c9760a721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KNeighborsClassifier()"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# K-NN classifier with K=5\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm2d3-AWLAwY"
      },
      "source": [
        "Now, let us calculate the AUC values and compare the classifiers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JaCethYLAwZ"
      },
      "outputs": [],
      "source": [
        "# look at metrics (AUC)\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "log_auc = roc_auc_score(y_test, log_fit.predict_proba(X_test)[:,1])\n",
        "dt_auc = roc_auc_score(y_test, dt.predict_proba(X_test)[:,1])\n",
        "rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:,1])\n",
        "gb_auc = roc_auc_score(y_test, gb.predict_proba(X_test)[:,1])\n",
        "knn_auc = roc_auc_score(y_test, knn.predict_proba(X_test)[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx6d0RDeLAwZ",
        "outputId": "01842deb-291d-476a-fc13-9524de72728b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier            AUC (Default)\n",
            "------------------  ---------------\n",
            "Penalized Logistic         0.647664\n",
            "Decision Tree              0.590428\n",
            "Random Forest              0.734655\n",
            "Boosting Tree              0.733306\n",
            "k-NN                       0.629671\n"
          ]
        }
      ],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# create table data\n",
        "table_data = [[\"Penalized Logistic\", log_auc], \n",
        "        [\"Decision Tree\", dt_auc], \n",
        "        [\"Random Forest\", rf_auc], \n",
        "        [\"Boosting Tree\", gb_auc],\n",
        "        [\"k-NN\", knn_auc]]\n",
        "  \n",
        "# define header names\n",
        "col_names = [\"Classifier\", \"AUC (Default)\"]\n",
        "  \n",
        "# display table\n",
        "print(tabulate(table_data, headers=col_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3EOlZoaLAwZ"
      },
      "source": [
        "Here we can see that the Random Forest and the Boosting Tree classifiers performed the best as they have the highest AUC values (both approx 0.73). The classifer that has the lowest AUC value is the Decision Tree, which only had an AUC value of 0.59. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6V7R282LAwa"
      },
      "source": [
        "#### 7. Default parameters are not necessarily the best for our database. I like this example provided by Will Koehrsen on Towardsdatascience \"The best way to think about hyperparameters is like the settings of an algorithm that can be adjusted to optimize performance, just as we might turn the knobs of an AM radio to get a clear signal\". So let us tune the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD5sCKHpLAwa"
      },
      "source": [
        "#### a. From your understanding of the class, identify at least two key tuning parameters for each method of question 6.\n",
        "\n",
        "The following lists at least two key tuning parameters for each method of Question 6.\n",
        "\n",
        "Penalized Logistic Classifier\n",
        "1. Solver (solver)\n",
        "2. Penalty (penalty)\n",
        "3. Penalty strength (C)\n",
        "\n",
        "Decision Tree Classifier \n",
        "1. Split criteria (criterion)\n",
        "2. Strategy used to choose the split at each node (splitter)\n",
        "3. Maximum depth of tree (max_depth)\n",
        "\n",
        "Random Forest classifier \n",
        "1. Number of trees (n_estimators)\n",
        "2. Number of random features to sample at each split point (max_features)\n",
        "\n",
        "\n",
        "Boosting Tree Classifier\n",
        "1. Learning rate (learning_rate)\n",
        "2. Number of trees (n_estimators)\n",
        "3. Number of rows or subset of the data to consider for each tree (subsample)\n",
        "4. Depth of each tree (max_depth)\n",
        "\n",
        "K-NN Classifier with K=5\n",
        "1. Number of neighbours (n_neighbours)\n",
        "2. Distance metric (metric)\n",
        "3. Contribution of members (weight)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B5otwGuLAwa"
      },
      "source": [
        "#### b. Using gridSearchCV, find the best parameters for each of the methods. Note that it will imply that you provide a grid for each parameter over which you will search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Onb8I8LAwb"
      },
      "source": [
        "Now, we can use use GridSearchCV to find the best parameters for each of the methods we have been working with. Using the list of tuning parameters from part (a), we can specify a grid for each parameter over which we will search. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu5g5vf8LAwb"
      },
      "source": [
        "#### Penalized Logistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U09jt-2ZLAwb",
        "outputId": "4378c9f5-de84-4ffb-da84-c62ad8023029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.752641 using {'C': 100, 'penalty': 'l1', 'solver': 'liblinear'}\n"
          ]
        }
      ],
      "source": [
        "# use gridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# penalized logistic classifier \n",
        "model = LogisticRegression()\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "penalty = ['l2','l1']\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "log_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
        "log_result = log_search.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (log_result.best_score_, log_result.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoZ8YGgdLAwc"
      },
      "source": [
        "#### Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrANsi1pLAwc",
        "outputId": "2c0ac68f-ff9e-4509-8bb9-36f86f9bb18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.701494 using {'criterion': 'entropy', 'max_depth': 3, 'splitter': 'best'}\n"
          ]
        }
      ],
      "source": [
        "# decision tree classifier \n",
        "\n",
        "# define models and parameters\n",
        "model = DecisionTreeClassifier()\n",
        "criterion = ['gini','entropy']\n",
        "splitter = ['best','random']\n",
        "max_depth = [3,7,9]\n",
        "\n",
        "# define grid search\n",
        "grid = dict(criterion=criterion,splitter=splitter,max_depth=max_depth)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "dt_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
        "dt_result = dt_search.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (dt_result.best_score_, dt_result.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XuIsE-XLAwc"
      },
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaskaUATLAwd",
        "outputId": "d4fa784e-8e35-472f-8e8f-af6625b825e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.794525 using {'max_features': 'log2', 'n_estimators': 1000}\n"
          ]
        }
      ],
      "source": [
        "# random forest classifier \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# define models and parameters\n",
        "model = RandomForestClassifier()\n",
        "n_estimators = [10, 100, 1000]\n",
        "max_features = ['sqrt', 'log2']\n",
        "\n",
        "# define grid search\n",
        "grid = dict(n_estimators=n_estimators,max_features=max_features)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "rf_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
        "rf_result = rf_search.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (rf_result.best_score_, rf_result.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rm_-hctLAwd"
      },
      "source": [
        "#### Boosting Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKVSgKnrLAwe",
        "outputId": "b0b92b70-575c-488a-cda2-d791d2443ae8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.787957 using {'learning_rate': 0.001, 'max_depth': 9, 'n_estimators': 1000, 'subsample': 0.5}\n"
          ]
        }
      ],
      "source": [
        "# boosting tree classifier \n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# define models and parameters\n",
        "model = GradientBoostingClassifier()\n",
        "n_estimators = [10, 100, 1000]\n",
        "learning_rate = [0.001, 0.01, 0.1]\n",
        "subsample = [0.5, 0.7, 1.0]\n",
        "max_depth = [3, 7, 9]\n",
        "\n",
        "# define grid search\n",
        "grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "gb_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
        "gb_result = gb_search.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (gb_result.best_score_, gb_result.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tibhp_SlLAwf"
      },
      "source": [
        "#### K-NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HtsOGZNLAwf",
        "outputId": "f35320a0-584f-471a-ae71-707169ceda19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.758475 using {'metric': 'manhattan', 'n_neighbors': 19, 'weights': 'distance'}\n"
          ]
        }
      ],
      "source": [
        "# K-NN classifier with K=5 \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# define models and parameters\n",
        "model = KNeighborsClassifier()\n",
        "n_neighbors = range(1, 21, 2)\n",
        "weights = ['uniform', 'distance']\n",
        "metric = ['euclidean', 'manhattan', 'minkowski']\n",
        "\n",
        "# define grid search\n",
        "grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "knn_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc',error_score=0)\n",
        "knn_result = knn_search.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (knn_result.best_score_, knn_result.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icvR-TiILAwg"
      },
      "source": [
        "#### c. Complete the table in the previous section by comparing the gain in terms of accuracy from the tuning procedure vs. the default parameters procedures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1i3ca733LAwh"
      },
      "outputs": [],
      "source": [
        "# look at metrics (AUC)\n",
        "log_auc_tuned = log_result.best_score_\n",
        "dt_auc_tuned = dt_result.best_score_\n",
        "rf_auc_tuned = rf_result.best_score_\n",
        "gb_auc_tuned = gb_result.best_score_\n",
        "knn_auc_tuned = knn_result.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HreMyYujLAwh",
        "outputId": "38e6de5d-6adf-4527-a88b-ee2715fc6c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classifier            AUC (Default)    AUC (Tuned)    Difference\n",
            "------------------  ---------------  -------------  ------------\n",
            "Penalized Logistic         0.647664       0.752641     0.104977\n",
            "Decision Tree              0.590428       0.701494     0.111066\n",
            "Random Forest              0.734655       0.794525     0.0598702\n",
            "Boosting Tree              0.733306       0.787957     0.0546507\n",
            "k-NN                       0.629671       0.758475     0.128804\n"
          ]
        }
      ],
      "source": [
        "# create table data\n",
        "table2_data = [[\"Penalized Logistic\", log_auc, log_auc_tuned, log_auc_tuned-log_auc], \n",
        "        [\"Decision Tree\", dt_auc, dt_auc_tuned, dt_auc_tuned-dt_auc], \n",
        "        [\"Random Forest\", rf_auc, rf_auc_tuned, rf_auc_tuned-rf_auc], \n",
        "        [\"Boosting Tree\", gb_auc, gb_auc_tuned, gb_auc_tuned-gb_auc],\n",
        "        [\"k-NN\", knn_auc, knn_auc_tuned, knn_auc_tuned-knn_auc]]\n",
        "  \n",
        "# define header names\n",
        "col_names2 = [\"Classifier\", \"AUC (Default)\", \"AUC (Tuned)\", \"Difference\"]\n",
        "  \n",
        "# display table\n",
        "print(tabulate(table2_data, headers=col_names2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RP37ezIzLAwi"
      },
      "source": [
        "Here we can see a table with the AUC values for all of the classifiers for both the default and the tuned models. As we previously saw, the Random Forest and the Boosting Tree classifiers had the highest AUC values with the default parameters. After tuning the models, we see that they still have the highest values. However, we can note that there seemed to be significant differences in the AUC of the default and the tuned models for the other classifiers, namely the Penalized Logistic, Decision Tree, and k-NN. For these classifiers, the difference in AUC between the default and the tuned models was between 0.10-0.13. This is quite the difference contrasted to the differences of approx 0.5-0.6 that the Random Forest and Boosting Tree classifiers saw.\n",
        "\n",
        "We note that the Random Forest and Boosting Tree classifiers had good AUCs to begin with, and the GridSearch is costly in time. For the other classifiers, it is worth it as we see great improvements. Now, all of the tuned AUC values are in the 0.75-0.79 range, which is pretty good for a classifier. This indicates that the models are performing working well.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLVbSszYLAwi"
      },
      "source": [
        "#### 8. Other variables could matter for the prediction. I am thinking in particular about growth variables. Compute the growth of all the variables in the database (you can help yourself by using Stata at this stage or Matlab or any other software with which you feel more comfortable calculating growth rates).\n",
        "\n",
        "Growth rates for this question will be calculated in R. The code will be available in the .rmd file. \n",
        "\n",
        "To calculate growth rate, we will start by subtracting the past value from the current value. Then, we divide that number by the past value. Finally, we can multiply our answer by 100 to express it as a percentage. It is important to note that these growth rates must be calculated by country and year. \n",
        "\n",
        "Let us export our dataset with imputed values to read into R."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cst_QxBRLAwj"
      },
      "outputs": [],
      "source": [
        "crisis.to_csv(\"crisis_ohe_imputed.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2P2AjpnLAwl"
      },
      "source": [
        "#### 9. Using the most correlated variables (similar to what we have seen in Ferrara-Simoni 2019), select the variable that will enter your model and do the prediction using a lasso-logit. Note that you should consider two tuning parameters in this case: the degree of the penalty and the number of correlated variables to consider. Why might selecting the most correlated variables matter?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2qhJI5PLAwl"
      },
      "source": [
        "Now, let us read in our new dataset with calculated growth rates from question 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4qOPbaCJLAwm",
        "outputId": "ec543432-7ac7-4d47-ba35-497ff2596015"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>crisis</th>\n",
              "      <th>abs_corr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>crisis</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>credit2privat</th>\n",
              "      <td>-0.132380</td>\n",
              "      <td>0.132380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>int_exp2rev</th>\n",
              "      <td>0.121315</td>\n",
              "      <td>0.121315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fcd2g_dweo</th>\n",
              "      <td>0.109843</td>\n",
              "      <td>0.109843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_91</th>\n",
              "      <td>0.107120</td>\n",
              "      <td>0.107120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ca2gdp</th>\n",
              "      <td>-0.106505</td>\n",
              "      <td>0.106505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>year</th>\n",
              "      <td>-0.092141</td>\n",
              "      <td>0.092141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gfn2gdp</th>\n",
              "      <td>0.090052</td>\n",
              "      <td>0.090052</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>grossdebt2gdpweo</th>\n",
              "      <td>0.084956</td>\n",
              "      <td>0.084956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_4</th>\n",
              "      <td>0.077788</td>\n",
              "      <td>0.077788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_97</th>\n",
              "      <td>0.075741</td>\n",
              "      <td>0.075741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_78</th>\n",
              "      <td>0.072959</td>\n",
              "      <td>0.072959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_115</th>\n",
              "      <td>0.067557</td>\n",
              "      <td>0.067557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_39</th>\n",
              "      <td>0.058409</td>\n",
              "      <td>0.058409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_31</th>\n",
              "      <td>0.055331</td>\n",
              "      <td>0.055331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_100</th>\n",
              "      <td>0.054667</td>\n",
              "      <td>0.054667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_110</th>\n",
              "      <td>0.052491</td>\n",
              "      <td>0.052491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>inter_extdeb2gdp</th>\n",
              "      <td>0.050708</td>\n",
              "      <td>0.050708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_16</th>\n",
              "      <td>0.050476</td>\n",
              "      <td>0.050476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_53</th>\n",
              "      <td>0.045108</td>\n",
              "      <td>0.045108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_15</th>\n",
              "      <td>0.044917</td>\n",
              "      <td>0.044917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_2</th>\n",
              "      <td>0.044647</td>\n",
              "      <td>0.044647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_5</th>\n",
              "      <td>0.042951</td>\n",
              "      <td>0.042951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>debtserv2expor</th>\n",
              "      <td>0.042047</td>\n",
              "      <td>0.042047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_49</th>\n",
              "      <td>0.040918</td>\n",
              "      <td>0.040918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_62</th>\n",
              "      <td>0.038995</td>\n",
              "      <td>0.038995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_14</th>\n",
              "      <td>0.037172</td>\n",
              "      <td>0.037172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_64</th>\n",
              "      <td>0.037172</td>\n",
              "      <td>0.037172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_101</th>\n",
              "      <td>0.037172</td>\n",
              "      <td>0.037172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_75</th>\n",
              "      <td>0.032159</td>\n",
              "      <td>0.032159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>efn2gdp</th>\n",
              "      <td>-0.031421</td>\n",
              "      <td>0.031421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_3</th>\n",
              "      <td>0.030174</td>\n",
              "      <td>0.030174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>debtserv2res</th>\n",
              "      <td>0.029603</td>\n",
              "      <td>0.029603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stdebt2extdebt</th>\n",
              "      <td>0.028564</td>\n",
              "      <td>0.028564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_86</th>\n",
              "      <td>0.026556</td>\n",
              "      <td>0.026556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_112</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_113</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_111</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_103</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_102</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_35</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_94</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_89</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_29</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_92</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_37</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_93</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_36</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_28</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>country_38</th>\n",
              "      <td>-0.026162</td>\n",
              "      <td>0.026162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    crisis  abs_corr\n",
              "crisis            1.000000  1.000000\n",
              "credit2privat    -0.132380  0.132380\n",
              "int_exp2rev       0.121315  0.121315\n",
              "fcd2g_dweo        0.109843  0.109843\n",
              "country_91        0.107120  0.107120\n",
              "ca2gdp           -0.106505  0.106505\n",
              "year             -0.092141  0.092141\n",
              "gfn2gdp           0.090052  0.090052\n",
              "grossdebt2gdpweo  0.084956  0.084956\n",
              "country_4         0.077788  0.077788\n",
              "country_97        0.075741  0.075741\n",
              "country_78        0.072959  0.072959\n",
              "country_115       0.067557  0.067557\n",
              "country_39        0.058409  0.058409\n",
              "country_31        0.055331  0.055331\n",
              "country_100       0.054667  0.054667\n",
              "country_110       0.052491  0.052491\n",
              "inter_extdeb2gdp  0.050708  0.050708\n",
              "country_16        0.050476  0.050476\n",
              "country_53        0.045108  0.045108\n",
              "country_15        0.044917  0.044917\n",
              "country_2         0.044647  0.044647\n",
              "country_5         0.042951  0.042951\n",
              "debtserv2expor    0.042047  0.042047\n",
              "country_49        0.040918  0.040918\n",
              "country_62        0.038995  0.038995\n",
              "country_14        0.037172  0.037172\n",
              "country_64        0.037172  0.037172\n",
              "country_101       0.037172  0.037172\n",
              "country_75        0.032159  0.032159\n",
              "efn2gdp          -0.031421  0.031421\n",
              "country_3         0.030174  0.030174\n",
              "debtserv2res      0.029603  0.029603\n",
              "stdebt2extdebt    0.028564  0.028564\n",
              "country_86        0.026556  0.026556\n",
              "country_112      -0.026162  0.026162\n",
              "country_113      -0.026162  0.026162\n",
              "country_111      -0.026162  0.026162\n",
              "country_103      -0.026162  0.026162\n",
              "country_102      -0.026162  0.026162\n",
              "country_35       -0.026162  0.026162\n",
              "country_94       -0.026162  0.026162\n",
              "country_89       -0.026162  0.026162\n",
              "country_29       -0.026162  0.026162\n",
              "country_92       -0.026162  0.026162\n",
              "country_37       -0.026162  0.026162\n",
              "country_93       -0.026162  0.026162\n",
              "country_36       -0.026162  0.026162\n",
              "country_28       -0.026162  0.026162\n",
              "country_38       -0.026162  0.026162"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# read in new data with growth rates\n",
        "crisis2 = pd.read_csv(\"crisis_growth.csv\")\n",
        "crisis2 = crisis2.fillna(crisis2.mean())\n",
        "\n",
        "# look at correlations with the outcome variable\n",
        "corr=pd.DataFrame(crisis2[crisis2.columns[1:]].corr()['crisis'][:])\n",
        "corr['abs_corr'] = corr['crisis'].abs()\n",
        "corr_sorted = corr.sort_values(by=['abs_corr'],ascending=False)\n",
        "\n",
        "corr_sorted.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlJavuD9LAwn"
      },
      "source": [
        "We are interested in selecting only the variables that are highly correlated with our outcome variable, crisis. Looking at the output above, we see that the top 5 most correlated variables are credit2privat, int_exp2rev, fcd2g_dweo, ca2gdp, and year. This is interesting as none of the growth variables we calculated appeared near the top, the closest one being debtserv2expor_growth with an absolute correlation of 0.017776. \n",
        "\n",
        "Note that the country variables appear near the top as well. When dealing with categorical variables and model selection, we should not only include one \n",
        "level of the variable and not the others. Either we include all levels, or none. It is important to treat all of these dummy variables as a whole. In essence, by only including one level of the categorical variable, we can now changed the original variable, as we will no longer be able to refer back to the baseline. Because of this, I will not include single country variables.\n",
        "\n",
        "Let us first try the model with the top 5 correlated variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3H2nFknLAwn",
        "outputId": "a108644e-2049-40c0-df36-5ea7031b9da5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6982236842105264"
            ]
          },
          "execution_count": 283,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# prediction using LASSO logit\n",
        "train2 = crisis2[crisis2['year']<=2009]\n",
        "test2 = crisis2[crisis2['year']>2009]\n",
        "\n",
        "X_train5 = train2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp', 'year']]\n",
        "X_test5 = test2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp', 'year']]\n",
        "\n",
        "# scale the data \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train5 = scaler.fit_transform(X_train5)\n",
        "X_test5 = scaler.transform(X_test5)\n",
        "\n",
        "log5 = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "log5.fit(X_train5, y_train)\n",
        "\n",
        "# look at AUC\n",
        "from sklearn import metrics\n",
        "log_auc5 = roc_auc_score(y_test, log5.predict_proba(X_test5)[:,1])\n",
        "log_auc5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLwKqPapLAwo"
      },
      "source": [
        "Now, let us try with the top 7 correlated variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r5rVe3DLAwp",
        "outputId": "fe78e28c-2541-4148-c82e-b80df7e2d3c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7084210526315788"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train7 = train2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp','year','gfn2gdp','grossdebt2gdpweo']]\n",
        "X_test7 = test2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp', 'year','gfn2gdp','grossdebt2gdpweo']]\n",
        "\n",
        "# scale the data \n",
        "scaler = StandardScaler()\n",
        "X_train7 = scaler.fit_transform(X_train7)\n",
        "X_test7 = scaler.transform(X_test7)\n",
        "\n",
        "log7 = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "log7.fit(X_train7, y_train)\n",
        "\n",
        "# look at AUC\n",
        "log_auc7 = roc_auc_score(y_test, log7.predict_proba(X_test7)[:,1])\n",
        "log_auc7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8UH-u8TLAwq"
      },
      "source": [
        "Finally, we can try with the top 10 correlated variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKXcsgSoLAwq",
        "outputId": "b168ee68-ebbc-4897-e777-9eda61e4754e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6995394736842105"
            ]
          },
          "execution_count": 285,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train10 = train2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp','year','gfn2gdp','grossdebt2gdpweo','inter_extdeb2gdp','debtserv2expor','efn2gdp']]\n",
        "X_test10 = test2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp', 'year','gfn2gdp','grossdebt2gdpweo','inter_extdeb2gdp','debtserv2expor','efn2gdp']]\n",
        "\n",
        "# scale the data \n",
        "scaler = StandardScaler()\n",
        "X_train10 = scaler.fit_transform(X_train10)\n",
        "X_test10 = scaler.transform(X_test10)\n",
        "\n",
        "log10 = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "log10.fit(X_train10, y_train)\n",
        "\n",
        "# look at AUC\n",
        "log_auc10 = roc_auc_score(y_test, log10.predict_proba(X_test10)[:,1])\n",
        "log_auc10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3xcepCLAwr"
      },
      "source": [
        "Here we can see that the model with the top 7 correlated variables has the highest AUC value of 0.708. Thus, we can keep the model with the top 7 correlated variables.\n",
        "\n",
        "Selecting the most correlated variables is important as we are interested in finding the variables that best predict the outcomes. One issue with the approach taken here (looking soley at correlation coefficients), is that issues like multicollinearity may arise. An alternative method would be to run a series of regressions, regressing each individual independent variable on the outcome variable, crisis, and then examining the t-statistics/F-scores. By looking at the significant results, we can then choose the predictors to enter our model. Additionally, there are other tests that can be used when trying to compare models such as the likelihood ratio test (LRT) and residual deviance tests.\n",
        "\n",
        "In general, just looking at correlated variables as a way of model selection is not always sufficient. One should be careful when using different techniques for model selection. Other methods include automatic selection procedures such as forward/backward subset selection. When selecting a model, these procedures and techniques should be used with caution and in combination with domain specific knowledge. Moreover, one must keep in mind if they are more interested in using the model for inference or prediction. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEuwv6wPLAws"
      },
      "source": [
        "#### 10. Because the cost of missing a crisis is high, governments worry more about it. What does this mean for your accuracy metric? Redo the setting from question 9. Comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZpjPFB6LAwv"
      },
      "source": [
        "Because the cost of missing a crisis is high, we want to make sure to try and minimize false negatives. A false negative is an outcome where the model incorrectly predicts the negative class. In our context, a false negative is when the model predicts no crisis, when in reality, there is one. \n",
        "\n",
        "Recall the concepts of type I and II errors. A type I error occurs when the null hypothesisis true, but is rejected (false positive). A type II error occurs when the null hypothesis is false, but erroneously fails to be rejected (false negative).\n",
        "\n",
        "With this in mind, a metric that we can try to maximize then is recall. Recall is the number of true positives divided by the total number of elements that actually belong to the positive class. This is also known as sensitivity or the TPR, as previously seen in question 4.\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{TP}{TP+FN}\n",
        "$$\n",
        "\n",
        "More generally, recall is simply the complement of the type II error rate, i.e. one minus the type II error rate. Thus, to maximize recall, we must minimize type II error, and in turn minimize our false negatives.\n",
        "\n",
        "Therefore, we can redo the setting from question 9, but this time using recall as our accuracy metric. Let us do some hyperparameter tuning to find the optimal model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3tggCDWLAww",
        "outputId": "4853c262-cbcc-4e5c-e84e-51efb90525b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.000000 using {'C': 100, 'solver': 'liblinear'}\n"
          ]
        }
      ],
      "source": [
        "# redo question 9, using recall as accuracy metric \n",
        "\n",
        "# use top 5 correlated variables\n",
        "X_train7 = train2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp','year','gfn2gdp','grossdebt2gdpweo']]\n",
        "X_test7 = test2[['credit2privat', 'int_exp2rev', 'fcd2g_dweo','ca2gdp', 'year','gfn2gdp','grossdebt2gdpweo']]\n",
        "\n",
        "# scale the data \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train7 = scaler.fit_transform(X_train7)\n",
        "X_test7 = scaler.transform(X_test7)\n",
        "\n",
        "log7 = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "log7.fit(X_train7, y_train)\n",
        "\n",
        "# use gridSearchCV\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# penalized logistic classifier \n",
        "model = LogisticRegression(penalty='l1')\n",
        "solvers = ['liblinear'] #note that newton-cg and lbfgs only supports l2 penalty or none\n",
        "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
        "\n",
        "# define grid search\n",
        "grid = dict(solver=solvers,C=c_values)\n",
        "cv = RepeatedStratifiedKFold(n_splits=10,  n_repeats=3, random_state=1)\n",
        "log7_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring = 'recall',error_score=0)\n",
        "log7_result = log_search.fit(X_train7, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (log7_result.best_score_, log7_result.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPsIjAucLAwx"
      },
      "source": [
        "Here we see that the best parameters are C = 100, and the liblinear solver. Using recall as our accuracy measure, we find that we get a very poor/low recall value. Intuitively, I suppose that this is due to the fact that the dataset is very imbalanced towards the zero class, thus maximizing the recall puts all the predictions there since it has a lot more samples. I discuss other ways to further improve the accuracy of the prediction in the next question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJW9b8PLLAwy"
      },
      "source": [
        "#### 11. How could you further improve the accuracy of the prediction?\n",
        "\n",
        "As with any analysis, it is always good to consider how we can improve our model. In order to further improve the accuracy of the prediction, there are many steps we can take all throughout the model development process. Here I will name a few:\n",
        "\n",
        "**Adding more data**\n",
        "- If possible, we can try to collect more data points. This will allow the data to \"speak for itself\", so to speak. Having more data in our dataset will help the model train more accurately by capturing a larger portion of the data distribution.\n",
        "- Moreover, there may be different variables that we can add to our model that we do not yet have.  \n",
        "    \n",
        "**Deal with missing data differently** \n",
        "- The presence of missing values can reduce the accuracy of the model or lead to a biased model and inaccurate predictions. This is because we don’t analyze the behavior and relationship with other variables correctly.\n",
        "- As mentioned in question 5, there are alternative ways of dealing with missing values. It could be of value to try out different, more complex/precise ways of dealing with missing values instead of the way that was done here. \n",
        "- For example, different types of deterministic imputation methods include logical imputation, historical (e.g. carry-forward) imputation, mean imputation, ratio and regression imputation. We could also try imputation with an additional column to signifiy if a value has been imputed or not.  \n",
        "\n",
        "**Treat outliers**\n",
        "- Another thing we can do is try and deal with outliers in the data.\n",
        "- We can delete the observations, perform transformation, binning, impute, or treat outlier values separately.\n",
        "\n",
        "**Derive and transform variables**\n",
        "- This step can also be thought of as feature engineering.\n",
        "- We can try and see if there are any derived variables we can create using the data we have. This usually goes with domain knowledge, as well as thinking about the original research question.\n",
        "- Transforming variables such as scaling can help as well. In this exercise, we scaled using standardization. We can also try the MinMaxScaler() in a future analysis to put the data point on the (0,1) interval.  \n",
        "\n",
        "**Model selection**\n",
        "- We can use different model selection techniques to try and find the best predictors to add to our model. \n",
        "- We can use principal components analysis, which is a dimensionality reduction technique that helps to represent training data into lower dimensional spaces, but still characterize the inherent relationships in the data.\n",
        "- As discussed briefly in question 9,  an alternative method would be to run a series of regressions, regressing each individual independent variable on the outcome variable, crisis, and then examining the t-statistics/F-scores. We could also use automatic selection procedures such as backwards/forwards subset selection, or use statistical tests like LRT or residual deviance to compare models. \n",
        "- Again, it is crucial to keep in mind that when selecting a model, these procedures and techniques should be used with caution and in combination with domain specific knowledge.  \n",
        "\n",
        "**Hyperparameter tuning**\n",
        "- We can also try to tune more hyperparamters. Some classiers we can tune more hyperparameters, as we saw in question 7a, there are lots of different parameters we can tune, some classifiers more than others.\n",
        "- Moreover, we can also try using different other types of classifiers.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeKNXlaxLAw0"
      },
      "source": [
        "#### 12. Summarize your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h82IbdmkLAw1"
      },
      "source": [
        "**Summary**  \n",
        "For this analysis, we first started with our exploratory data analysis (EDA) by looking at discriptive statistics of our variable of interests. After that, we did some data preprocessing which involved things such as dealing with missing values, variable endcoding, and standardization. Then, we split the data into training and testing sets to be used to train and evaluate our model. \n",
        "\n",
        "We used 5 different classifiers, namely, penalized logistic, decision tree classifier, random forest classifier, boosting tree, and K-NN classifier (with K=5). We first fit the models with the default parameters and found that the random forest and the boosting tree performed the best using AUC as our ccuracy metric. We then repeated the analysis, this time using GridSearchCV to tune our hyperparameters. Again, we found that the random forest and boosting tree had the highest AUC, but we saw considerable improvement in the other classifiers. \n",
        "\n",
        "Next, we calculated growth rates and reviewed the most correlated variables. Ultimately, we selected the top 7 correlated variables, i.e., credit2privat, int_exp2rev, fcd2g_dweo, ca2gdp, year, gfn2gdp, and grossdebt2gdpweo. We then repeated the exercise using recall as our accuracy metric as we wanted to minimize false negatives (missing a crisis), and to do so, we wanted to minimize type II errors (maximize recall). For this analysis, we had a very bad value of recall, which could be attributed to our inbalanced dataset. Further work can be done to improve upon this analysis as we discussed in question 11 above. \n",
        "\n",
        "**Reflection**  \n",
        "Now, I will reflect on the analysis and discuss what went well, and what didn't. There are a few key take-aways that I got from this modelling exercise, namely:\n",
        "\n",
        "- In practice, dealing with missing values is not easy! The decisions you make on how to treat such values can greatly affect how the algorithm will perform and how we represent the data. More generally, data preparation is an intensive task that is very important, and must be done before we even get to start modelling.\n",
        "- Modelling, and machine learning, is an art. There are usually multiple ways of doing things right, and given a set to analyze, each individual's path will not be same. Hyperparamter tuning is an iterative process, and we should be mindful of all the decisions we make and what their assumptions/implications are in every step of the model development process. \n",
        "- Things should always be taken in context. We must be mindful of the data we are working with, what our goal is, how was this data collected, how does this relate to our hypothesis, etc. "
      ]
    }
  ]
}